{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinions and Gaze: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the gaze and questionnaire data processing for \"Seeing the other side: Conflict and controversy increase gaze coordination\" (Paxton, Dale, & Richardson, *in preparation*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this file from scratch, you will need:\n",
    "* `data/`: Directory of experiment data, available for download from the project's ICPSR directory (link here). Due to data sensitivity (per the Institutional Review Board of the University of California, Merced), only researchers from ICPSR member institutions may access the data.\n",
    "    * `listener-gaze-raw/*.txt`: Listeners' raw gaze data\n",
    "    * `listener-responses-raw/`:\n",
    "        * `*.xml`: Listeners' trial order during experiment\n",
    "        * `*.tsv`: Listeners' post-experiment questionnaire data\n",
    "    * `speaker-gaze-raw/*.txt`: Speakers' raw gaze data\n",
    "    * `speaker-audio_outputs/*.csv`: Speaker's audio segment timing\n",
    "    * `speaker-segment_key.csv`: List of which speakers provided which stimulus segments\n",
    "* `supplementary-code/`: Directory of additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Listener gaze data](#Listener-gaze-data)\n",
    "    * [Convert listeners' raw SMI files](#Convert-listeners'-raw-SMI-files)\n",
    "    * [Concatenate multifile data from single listeners](#Concatenate-multifile-data-from-single-listeners)\n",
    "    * [Segment listeners' files by audio clip](#Segment-listeners'-files-by-audio-clip)\n",
    "* [Listener survey response data](#Listener-survey-response-data)\n",
    "* [Speaker gaze data](#Speaker-gaze-data)\n",
    "    * [Convert speakers' raw SMI files](#Convert-speakers'-raw-SMI-files)\n",
    "    * [Filter speaker gaze data to relevant topic](#Filter-speaker-gaze-data-to-relevant-topic)\n",
    "* [Clean up interim directories](#Clean-up-interim-directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written by**: A. Paxton\n",
    "<br>**Date last modified**: 21 February 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in bespoke functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-clean_responses.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-stimulus_order.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-clean_gaze_data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listener gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert listeners' raw SMI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all of our raw gaze data\n",
    "gazeData = glob.glob('../data/listener-gaze-raw/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "if not os.path.exists('../data/listener-gaze-prepped/'):\n",
    "    os.makedirs('../data/listener-gaze-prepped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process each file\n",
    "for gazeFile in gazeData: clean_gaze_data(gazeFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate multifile data from single listeners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all our processed files' names\n",
    "processed_files = glob.glob('../data/listener-gaze-prepped/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify files with longer names than expected\n",
    "possible_multipart_files = [re.findall('\\d{5}', ID)[0] for ID \n",
    "                            in processed_files if len(ID)>48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify possible multipart files\n",
    "multipart_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in possible_multipart_files]\n",
    "multipart_ids = [re.findall('\\d{5}', mpf)[0] for mpf in multipart_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many times these odd IDs appear in our files\n",
    "from collections import Counter\n",
    "single_ids = [ID for ID in Counter(multipart_ids) if \n",
    "              Counter(multipart_ids)[ID]==1]\n",
    "multi_ids = [ID for ID in Counter(multipart_ids) if \n",
    "              Counter(multipart_ids)[ID]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the IDs that only occur once\n",
    "single_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in single_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the oddly named files with unique IDs\n",
    "for single_file in single_files:\n",
    "    participant_id = re.findall('\\d{5}', single_file)[0]\n",
    "    new_file_name = os.path.join('../data/listener-gaze-prepped',\n",
    "                                 participant_id+'-smi-data.csv')\n",
    "    os.rename(single_file,new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the multi-part participant files\n",
    "multi_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in multi_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through all possibly duplicated participants\n",
    "for duplicate_id in multi_ids:\n",
    "    \n",
    "    # identify which files belong to this person\n",
    "    target_files = [next_file for next_file in processed_files if\n",
    "                      re.findall('\\d{5}', next_file)[0] == duplicate_id]\n",
    "    \n",
    "    # concatenate the files\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    for next_file in target_files:\n",
    "        concatenated_df = concatenated_df.append(\n",
    "                                            pd.read_csv(next_file)\n",
    "                                        ).reset_index(drop=True)\n",
    "        \n",
    "    # delete the old files\n",
    "    for next_file in target_files:\n",
    "        os.remove(next_file)\n",
    "        \n",
    "    # if there are any duplicated rows, just keep the first\n",
    "    concatenated_df = concatenated_df.drop_duplicates()\n",
    "    \n",
    "    # save the new file\n",
    "    new_concat_name = os.path.join('../data/listener-gaze-prepped',\n",
    "                                     duplicate_id+'-smi-data.csv')\n",
    "    concatenated_df.to_csv(new_concat_name, sep=',', \n",
    "                           header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment listeners' files by audio clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "if not os.path.exists('../data/listener-gaze-cleaned/'):\n",
    "    os.makedirs('../data/listener-gaze-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up cleaned ones and take a peek at what's going on\n",
    "gazeCleaned = glob.glob('../data/listener-gaze-prepped/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which stimuli only had 1 audio clip\n",
    "single_stimuli = ['abortion','gay-marriage','legal-marijuana','tax-rich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cycle through the participants' data\n",
    "for nextGaze in gazeCleaned:\n",
    "    \n",
    "    # grab the next set of gaze data\n",
    "    gaze_df = pd.read_csv(nextGaze,sep=',')\n",
    "    particID = re.findall('\\d{5}',nextGaze)[0]\n",
    "    \n",
    "    # identify which stimuli are present in this participant's data\n",
    "    available_stimuli = list(set(gaze_df['Stimulus']))\n",
    "    pic_stimuli = [stim for stim in available_stimuli \n",
    "                       if len(re.findall(' Page',str(stim)))>0]\n",
    "    \n",
    "    # cycle through the available picture data\n",
    "    for next_pic in pic_stimuli:\n",
    "        \n",
    "        # grab the next picture data and the correct name for it\n",
    "        base_stim_name = next_pic.split(' ')[0]\n",
    "        instr_and_gaze_subset = gaze_df[gaze_df['Stimulus'].\\\n",
    "                                            str.contains(base_stim_name)\\\n",
    "                                       ].reset_index(drop=True)\n",
    "        gaze_subset = gaze_df[gaze_df['Stimulus'].\\\n",
    "                                  str.contains(next_pic)\\\n",
    "                             ].reset_index(drop=True)\n",
    "        \n",
    "        # if it's a unimodal issue, we don't have to worry about finding the second listening section\n",
    "        if base_stim_name in single_stimuli:\n",
    "            gaze_save_name = os.path.join('../data/listener-gaze-cleaned/gaze_trial-'\n",
    "                                          +base_stim_name+'-'+particID+'.csv')\n",
    "            gaze_subset.to_csv(gaze_save_name, index=False)\n",
    "        \n",
    "        # if it is part of a two-opinion topic, make sure we have both topics, and then slice them\n",
    "        elif len(instr_and_gaze_subset['Stimulus'].unique())>2:\n",
    "            \n",
    "            # get first appearance of each stimulus\n",
    "            first_appearance = instr_and_gaze_subset.\\\n",
    "                                        groupby('Stimulus').\\\n",
    "                                        first().\\\n",
    "                                        sort_values(by='Time',ascending=True).\\\n",
    "                                        reset_index()\n",
    "            \n",
    "            # index the last time the participant saw the \"for\" and \"against\" instructions\n",
    "            against_instr = first_appearance['Time']\\\n",
    "                                [first_appearance['Stimulus'].\\\n",
    "                                         str.contains('-against.rtf')\\\n",
    "                                ].item()\n",
    "            for_instr = first_appearance['Time']\\\n",
    "                                [first_appearance['Stimulus'].\\\n",
    "                                         str.contains('-for.rtf')\\\n",
    "                                ].item()\n",
    "\n",
    "            # if the \"against\" instruction came first\n",
    "            if against_instr < for_instr:\n",
    "                against_listening = gaze_subset[gaze_subset['Time'] < for_instr]\n",
    "                for_listening = gaze_subset[gaze_subset['Time'] > for_instr]\n",
    "            \n",
    "            # otherwise, the \"for\" instruction came first\n",
    "            else:\n",
    "                for_listening = gaze_subset.loc[gaze_subset['Time'] < against_instr]\n",
    "                against_listening = gaze_subset.loc[gaze_subset['Time'] > against_instr]\n",
    "                \n",
    "            # create the file names\n",
    "            against_file = os.path.join('../data/listener-gaze-cleaned/gaze_trial-'\n",
    "                                          +base_stim_name+'-against-'+particID+'.csv')\n",
    "            for_file = os.path.join('../data/listener-gaze-cleaned/gaze_trial-'\n",
    "                                          +base_stim_name+'-for-'+particID+'.csv')\n",
    "            \n",
    "            # and then print it\n",
    "            against_listening.to_csv(against_file,index=False)\n",
    "            for_listening.to_csv(for_file,index=False)\n",
    "        \n",
    "        # if we don't have them both, let us know\n",
    "        else:\n",
    "            print \"ERROR for ID \"+particID+\": Gaze Data for `\"+base_stim_name+\"` Not Found.\"\n",
    "    \n",
    "    # print update\n",
    "    print \"Participant \"+particID+\" Exported.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listener survey response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through listener raw data\n",
    "listener_folders = glob.glob('../data/listener-responses-raw/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "if not os.path.exists('../data/listener-responses-cleaned/'):\n",
    "    os.makedirs('../data/listener-responses-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab listener IDs associated with our gaze files\n",
    "unique_listeners = glob.glob('../data/listener-gaze-cleaned/*.csv')\n",
    "unique_listeners = list(set([re.findall('\\d{5}',f_name)[0] \n",
    "                             for f_name in unique_listeners]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our rating categories\n",
    "l_ratings = ['rat-emot','passionate','know','convince','agree','common']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cycle through the listeners to grab their questionnaire data\n",
    "missing_questionnaires = list()\n",
    "for listener in unique_listeners:\n",
    "\n",
    "    # identify the questionnaire and XML paths\n",
    "    q_tsv_data_file = glob.glob('../data/listener-responses-raw/'+listener+'*.tsv')\n",
    "    xml_data_file = glob.glob('../data/listener-responses-raw/'+listener+'*.xml')\n",
    "    \n",
    "    # if we've got a questionnaire, process it\n",
    "    if len(q_tsv_data_file)>0:    \n",
    "        \n",
    "        # if we only have 1 per participant...\n",
    "        if len(q_tsv_data_file)==1:\n",
    "            \n",
    "            # export TSV to a new file\n",
    "            q_csv_data_file = os.path.join('../data/listener-responses-cleaned/'\n",
    "                                   +listener+'_questionnaire.csv')\n",
    "            clean_q_df = clean_responses(listener,\n",
    "                                         q_tsv_data_file[0],\n",
    "                                         q_csv_data_file)\n",
    "            \n",
    "            # grab their XML data\n",
    "            task_xml_data_file = open(xml_data_file[0], 'r').read()\n",
    "            l_stimulus_order = stimulus_order(task_xml_data_file)\n",
    "            l_stimulus_order = [re.sub('\\-(for|against)','',stimulus) \n",
    "                                if re.sub('\\-(for|against)','',stimulus) in single_stimuli \n",
    "                                else stimulus \n",
    "                                for stimulus in l_stimulus_order]\n",
    "\n",
    "            # add the task order data to the dataframe\n",
    "            clean_q_df['Topic'] = 'none'\n",
    "            for rating in l_ratings:\n",
    "                clean_q_df['Topic'].\\\n",
    "                        loc[clean_q_df['Source']==rating] \\\n",
    "                                = l_stimulus_order[0:len(\\\n",
    "                                                     clean_q_df['Topic'].\\\n",
    "                                                         loc[clean_q_df['Source']==rating])]\n",
    "                \n",
    "            # then save it and report back\n",
    "            clean_q_df.to_csv(q_csv_data_file,index=False)\n",
    "            print('Listener ID '+str(listener)+' Questionnaire Data Exported.')\n",
    "            \n",
    "\n",
    "        # if we've got more than 1 file per participant...\n",
    "        else:\n",
    "            \n",
    "            # create an overarching dataframe and filename for the participant\n",
    "            combined_clean_q_df = pd.DataFrame()\n",
    "            q_csv_data_file = os.path.join('../data/listener-responses-cleaned/'\n",
    "                       +listener+'_questionnaire.csv')\n",
    "            \n",
    "            # preserve their file names when first processing\n",
    "            for q_file in q_tsv_data_file:\n",
    "\n",
    "                # get the requisite files\n",
    "                next_ID = re.sub('_questionnaire.tsv', '', \n",
    "                                         os.path.basename(q_file))\n",
    "                xml_data_file = os.path.join('../data/listener-responses-raw/'\n",
    "                                   +next_ID+'-stimulus-log.xml')\n",
    "                \n",
    "                # clean up the TSV but DO NOT save\n",
    "                clean_q_df = clean_responses(next_ID, q_file, None)\n",
    "                \n",
    "                # process the XML data\n",
    "                task_xml_data_file = open(xml_data_file, 'r').read()\n",
    "                l_stimulus_order = stimulus_order(task_xml_data_file)\n",
    "                l_stimulus_order = [re.sub('\\-(for|against)','',stimulus) \n",
    "                                    if re.sub('\\-(for|against)','',stimulus) in single_stimuli \n",
    "                                    else stimulus \n",
    "                                    for stimulus in l_stimulus_order]\n",
    "\n",
    "                # add the task order data to the dataframe\n",
    "                clean_q_df['Topic'] = 'none'\n",
    "                for rating in l_ratings:\n",
    "                    clean_q_df['Topic'].\\\n",
    "                            loc[clean_q_df['Source']==rating] \\\n",
    "                                    = l_stimulus_order[0:len(\\\n",
    "                                                         clean_q_df['Topic'].\\\n",
    "                                                             loc[clean_q_df['Source']==rating])]\n",
    "                \n",
    "                # if this isn't the first dataset, remove the demographic survey data\n",
    "                if combined_clean_q_df.size!=0:\n",
    "                    clean_q_df = clean_q_df[clean_q_df['Topic']!='none']\n",
    "\n",
    "                # add to master dataframe    \n",
    "                combined_clean_q_df = combined_clean_q_df.append(clean_q_df)\n",
    "\n",
    "            # save it and report back\n",
    "            combined_clean_q_df.to_csv(q_csv_data_file,index=False)\n",
    "            print 'Listener ID '+str(listener)+ ' Questionnaire Data Exported.'\n",
    "\n",
    "    # if it doesn't exist, let us know\n",
    "    else:\n",
    "        missing_questionnaires.append([listener])\n",
    "        print 'ERROR: Listener ID '+str(listener)+ ' Questionnaire Data Not Found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare speaker gaze data for only each target trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert speakers' raw SMI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab speakers' raw gaze data\n",
    "gazeData = glob.glob('../data/speaker-gaze-raw/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "if not os.path.exists('../data/speaker-gaze-prepped/'):\n",
    "    os.makedirs('../data/speaker-gaze-prepped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each file\n",
    "for gazeFile in gazeData: clean_gaze_data(gazeFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter speaker gaze data to relevant topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new target directory if it doesn't yet exist\n",
    "if not os.path.exists('../data/speaker-gaze-cleaned/'):\n",
    "    os.makedirs('../data/speaker-gaze-cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data for speaker audio clips\n",
    "segment_key = pd.read_table('../data/speaker-segment_key.csv',\n",
    "                           sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique speakers\n",
    "speaker_list = segment_key['speaker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cycle through the speakers\n",
    "for speaker in speaker_list:\n",
    "    \n",
    "    # figure out how many segments the speaker did\n",
    "    speaker_segments = segment_key[segment_key['speaker']==speaker]\n",
    "    \n",
    "    # read in the speaker's data\n",
    "    prepped_gaze = pd.read_csv('../data/speaker-gaze-prepped/'+\n",
    "                               str(speaker)+'-smi-data.csv')\n",
    "    all_audio = pd.read_csv('../data/speaker-audio_outputs/'+\n",
    "                              str(speaker)+'-winnowed_samples.csv')\n",
    "    all_audio['stim'] = all_audio['Stimulus'].replace(' Page 1','',regex=True)\n",
    "        \n",
    "    # cycle through the segments\n",
    "    for next_segment in range(0,speaker_segments.shape[1]):\n",
    "    \n",
    "        # grab the next row\n",
    "        next_segment = segment_key.iloc[next_segment]\n",
    "        topic = next_segment['topic']\n",
    "        side = next_segment['side']\n",
    "        speaker = str(speaker)\n",
    "\n",
    "        # grab only the times that correspond to our target trial\n",
    "        audio_times = all_audio[all_audio['stim']==topic]\n",
    "        start_time = min(audio_times['Time'])\n",
    "        end_time = max(audio_times['Time'])\n",
    "\n",
    "        # carve out the data between start_time and end_time and save only the columns we need\n",
    "        target_gaze = prepped_gaze.loc[prepped_gaze['Time']>=start_time]\n",
    "        target_gaze = target_gaze.loc[target_gaze['Time']<=end_time]\n",
    "        if topic in single_stimuli:\n",
    "            outname = os.path.join('../data/speaker-gaze-cleaned/gaze_speaker-'\n",
    "                                   +topic+'-'+speaker+'.csv')\n",
    "        else:\n",
    "            outname = os.path.join('../data/speaker-gaze-cleaned/gaze_speaker-'\n",
    "                                   +topic+'-'+side+'-'+speaker+'.csv')\n",
    "        # save the data\n",
    "        target_gaze.to_csv(outname,index=False)\n",
    "\n",
    "        # print out an update\n",
    "        print \"Speaker Data Exported: \"+topic+\" (\"+side+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up interim directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done, we can delete the interim files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('../data/speaker-gaze-prepped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('../data/listener-gaze-prepped/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
