{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinions and Gaze: Data Processing (Step 2 of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the data processing for \n",
    "for \"Seeing the other side: Conflict and controversy \n",
    "increase gaze coordination\" (Paxton, Dale, & Richardson, \n",
    "*in preparation*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the **second of three** notebooks for the\n",
    "\"Opinions and Gaze\" project. This must be run **after**\n",
    "the `oag-data_cleaning.ipynb` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook completely from scratch, you will need the following\n",
    "files: \n",
    "\n",
    "* `data/02-data_cleaning`: Directory of cleaned study data, produced by \n",
    "    `oag-data_cleaning.ipynb`.\n",
    "    * `listener-gaze-cleaned/*.csv`: Listeners' clean gaze data.\n",
    "    * `listener-responses-cleaned/*.csv`: Listeners' clean questionnaire data.\n",
    "    * `speaker-gaze-cleaned/*.csv`: Speakers'clean gaze data.\n",
    "* `supplementary-code/`: Directory of additional functions and global\n",
    "    variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Due to data sensitivity (per the Institutional Review Board of the \n",
    "University of California, Merced), only researchers from ICPSR \n",
    "member institutions may access study data through the approved link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Prepare listener and speaker data](#Prepare-listener-and-speaker data)\n",
    "    - [Downsample data](#Downsample-data)\n",
    "    - [Prepare speaker gaze data](#Prepare-speaker-gaze-data)\n",
    "    - [Prepare listener gaze data](#Prepare-listener-gaze-data)\n",
    "    - [Remove unusable segments](#Remove-unusable-segments)\n",
    "* [Cross-recurrence quantification analysis](#Cross-recurrence-quantification-analysis)\n",
    "    - [Calculate cross-recurrence between speakers and listeners](#Calculate-cross-recurrence-between-speakers-and-listeners)\n",
    "* [Create baseline data](#Create-baseline-data)\n",
    "* [Create analysis and plotting dataframes](#Create-analysis-and-plotting-dataframes)\n",
    "    - [Bind real and baseline data](#Bind-real-and-baseline-data)\n",
    "    - [Convert text to numeric](#Convert-text-to-numeric)\n",
    "    - [Center dummy-coded variables](#Center-dummy-coded-variables)\n",
    "    - [Remove NAs](#Remove-NAs)\n",
    "    - [Create polynomial time variables](#Create-polynomial-time-variables)\n",
    "    - [Save the dataset](#Save-the-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written by**: A. Paxton (University of California, Berkeley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date last modified**: 29 March 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clear the space\n",
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in the needed files and functions\n",
    "source('../supplementary-code/libraries_and_functions-oag.r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a folder for processed data, if it doesn't exist\n",
    "if (!dir.exists(processed_data_path)){\n",
    "    dir.create(processed_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify paths for cleaned data\n",
    "speaker_gaze_cleaned_path = file.path(cleaned_data_path,\n",
    "                                     'speaker-gaze-cleaned')\n",
    "listener_gaze_cleaned_path = file.path(cleaned_data_path,\n",
    "                                      'listener-gaze-cleaned')\n",
    "listener_responses_cleaned_path = file.path(cleaned_data_path,\n",
    "                                      'listener-responses-cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare listener and speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create downsampled time variable\n",
    "sampling_round_time = create_downsampled_time(sampling_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare speaker gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read in each speaker's gaze data, remove data\n",
    "for any segments that were not presented to listeners, downsample,\n",
    "and then export individual files for each segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to equipment failure, we do not have any\n",
    "usable speaker gaze data for `drinking-age-against`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a folder, if it doesn't exist\n",
    "speaker_dataframe_path = file.path(processed_data_path,\n",
    "                                   'speaker-dataframes')\n",
    "if (!dir.exists(speaker_dataframe_path)){\n",
    "    dir.create(speaker_dataframe_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of cleaned speaker files\n",
    "speaker_gaze_files = list.files(speaker_gaze_cleaned_path, \n",
    "                                recursive=FALSE, \n",
    "                                full.names=TRUE,\n",
    "                                pattern=\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframes for completed and missing speakers\n",
    "speaker_times = data.frame()\n",
    "all_missing_speakers = data.frame()\n",
    "\n",
    "# cycle through each speaker's file\n",
    "for (speaker_file in speaker_gaze_files){\n",
    "    \n",
    "    # identify metadata from file name\n",
    "    speaker_metadata = strapply(speaker_file,\n",
    "                                \"gaze_speaker-(.*).csv\", c)[[1]]\n",
    "    speaker_val = strapply(speaker_metadata,\n",
    "                       \"-([[:digit:]]{5})\", c)[[1]]\n",
    "    \n",
    "    # identify topic and side (if two-sided issue)\n",
    "    metadata_to_list = strsplit(speaker_metadata,'-')[[1]]\n",
    "    second_to_last_word = tail(strsplit(speaker_metadata,'-')[[1]],\n",
    "                               n=2)[1]\n",
    "    if (second_to_last_word=='for' | second_to_last_word=='against'){\n",
    "        topic_val = strapply(speaker_metadata,\n",
    "                         \"(.*)-(for|against)-[[:digit:]]{5}\", c)[[1]][1]\n",
    "        side_val = strapply(speaker_metadata,\n",
    "                        \"(.*)-(for|against)-[[:digit:]]{5}\", c)[[1]][2]\n",
    "        topic_and_side = paste(topic_val, '-', side_val, sep=\"\")\n",
    "    } else {\n",
    "        topic_val = strapply(speaker_metadata,\n",
    "                 \"(.*)-[[:digit:]]{5}\", c)[[1]][1]\n",
    "        side_val = 'only'\n",
    "        topic_and_side = topic_val\n",
    "    }\n",
    "\n",
    "    # read in file, add metadata, and fix time\n",
    "    gaze_data = read.csv(speaker_file, \n",
    "                         stringsAsFactors=FALSE) %>%\n",
    "        mutate(speaker = speaker_val) %>%\n",
    "        mutate(topic = topic_val) %>%\n",
    "        mutate(side = side_val) %>%\n",
    "        mutate(time = round((Time - min(Time))/1000000, \n",
    "                            sampling_round_time))\n",
    "\n",
    "    # rename old variables\n",
    "    setnames(gaze_data,\n",
    "             old=old_rename_columns, \n",
    "             new=new_rename_columns)\n",
    "\n",
    "    # figure out how many samples we should have\n",
    "    max_time = max(na.omit(gaze_data)$time)\n",
    "    total_downsampled_samples = max_time * sampling_hz\n",
    "    \n",
    "    # clean up the data\n",
    "    gaze_data = gaze_data %>% ungroup() %>%\n",
    "    \n",
    "        # keep fixations of real AOIs\n",
    "        dplyr::filter(r_event=='Fixation') %>%\n",
    "        dplyr::filter(r_aoi!='White Space' & r_aoi!='-') %>%\n",
    "\n",
    "        # remove duplicate time slices\n",
    "        group_by(time, speaker) %>%\n",
    "        slice(1) %>% ungroup() %>%\n",
    "\n",
    "        # drop unwanted columns\n",
    "        select(one_of(speaker_keep_columns))\n",
    "    \n",
    "    # clean up the gaze data only if we have usable data\n",
    "    gaze_datatable = as.data.table(gaze_data)\n",
    "    if (nrow(gaze_datatable)>0){\n",
    "\n",
    "        # create dataframe of all expected time slices\n",
    "        setkey(gaze_datatable, speaker, time)\n",
    "        time_slices = CJ(speaker_val,\n",
    "                         seq(0.0,\n",
    "                             max_time,\n",
    "                             by=sampling_fraction)) %>%\n",
    "            mutate(V2 = round(V2, sampling_round_time))\n",
    "\n",
    "        # pad the dataframe with NAs as needed\n",
    "        gaze_data = gaze_data %>% ungroup() %>%\n",
    "\n",
    "            # merge with time segments\n",
    "            full_join(., time_slices,\n",
    "                              by=c('speaker'='V1',\n",
    "                                   'time'='V2')) %>%\n",
    "            dplyr::arrange(time) %>%\n",
    "        \n",
    "            # keep the speaker, topic, and side info\n",
    "            mutate(speaker = speaker_val) %>%\n",
    "            mutate(topic = topic_val) %>%\n",
    "            mutate(side = side_val) %>%\n",
    "            \n",
    "            # replace all NA with 99\n",
    "            mutate_all(funs(replace(., \n",
    "                                    is.na(.), \n",
    "                                    99)))\n",
    "        \n",
    "        # yell at us if we have more than one row per slice\n",
    "        if (nrow(gaze_data)!=(total_downsampled_samples+1)){\n",
    "            print(paste(\"Speaker \", speaker_val,\n",
    "                        \", Topic `\", topic_val,\n",
    "                        \"`: Improper time slicing -- ERROR\",\n",
    "                    sep=\"\"))\n",
    "        }\n",
    "\n",
    "        # save cleaned dataframe to file\n",
    "        next_file_basename = paste0(speaker_val,'-',\n",
    "                                    topic_and_side,\n",
    "                                    '-gaze_df.csv')\n",
    "        next_file_name = file.path(speaker_dataframe_path,\n",
    "                                   next_file_basename)\n",
    "        write.table(gaze_data, next_file_name,\n",
    "                    append=FALSE, sep=\",\", row.names=FALSE)\n",
    "\n",
    "        # append speaker gaze data to dataframe\n",
    "        speaker_times = rbind.data.frame(speaker_times,\n",
    "                                         gaze_data)\n",
    "        \n",
    "        # print update\n",
    "        print(paste(\"Speaker \",speaker_val,\n",
    "                    \", Topic `\",topic_val,\n",
    "                    \"`: Data exported\",\n",
    "                    sep=\"\"))\n",
    "        \n",
    "    } else {\n",
    "        # print update\n",
    "        print(paste(\"Speaker \",speaker_val,\",\",\n",
    "                    \" Topic `\",topic_val,\n",
    "                    \"`: No data recorded -- ERROR\",\n",
    "                    sep=\"\"))\n",
    "        \n",
    "        # save missing to dataframe\n",
    "        all_missing_speakers = rbind.data.frame(\n",
    "                    all_missing_speakers,\n",
    "                    data.frame(speaker = speaker_val,\n",
    "                               topic = topic_val,\n",
    "                               side = side_val,\n",
    "                               r_event = 'all_missing'))\n",
    "    }\n",
    "}\n",
    "\n",
    "# save any completely missing speakers\n",
    "missing_speaker_path = file.path(processed_data_path,\n",
    "                                'speaker-missing_data.csv')\n",
    "write.table(all_missing_speakers, missing_speaker_path,\n",
    "            append=FALSE, sep=',', row.names=FALSE)\n",
    "\n",
    "# write everyone else's data\n",
    "speaker_time_path = file.path(speaker_dataframe_path,\n",
    "                              'speaker-master-df.csv')\n",
    "write.table(speaker_times, speaker_time_path,\n",
    "            append=FALSE, sep=',', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare listener gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we cycle through all of the listeners' data by\n",
    "segment, perform some basic data checks, downsample,\n",
    "and then export individual files for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a folder, if it doesn't exist\n",
    "listener_dataframe_path = file.path(processed_data_path,\n",
    "                                    'listener-dataframes')\n",
    "if (!dir.exists(listener_dataframe_path)){\n",
    "    dir.create(listener_dataframe_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grab all individual listener files\n",
    "listener_gaze_files = list.files(listener_gaze_cleaned_path, \n",
    "                                 recursive=FALSE, \n",
    "                                 full.names=TRUE,\n",
    "                                 pattern=\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grab speaker times\n",
    "speaker_master_df_path = file.path(speaker_dataframe_path,\n",
    "                                   'speaker-master-df.csv')\n",
    "speaker_df = read.table(speaker_master_df_path,\n",
    "                        header=TRUE, row.names=NULL, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identify unique speakers and topics from df\n",
    "speaker_segment_list = speaker_df %>%\n",
    "    group_by(speaker, topic, side) %>%\n",
    "    select(speaker, topic, side) %>%\n",
    "    distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of our missing data\n",
    "missing_listener_data = data.frame()\n",
    "missing_listener_file = file.path(processed_data_path,\n",
    "                                  'listener-missing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of our unbalanced monologues\n",
    "unbalanced_listeners = data.frame()\n",
    "unbalanced_listener_file = file.path(processed_data_path,\n",
    "                                     'listener-unbalanced_segments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of our listeners without opinions\n",
    "missing_opinion_listeners = data.frame()\n",
    "missing_opinion_file = file.path(processed_data_path,\n",
    "                                 'listener-missing_opinions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep track of our listeners without questionnaires\n",
    "missing_questionnaire_listeners = data.frame()\n",
    "missing_questionnaire_file = file.path(processed_data_path,\n",
    "                                       'listener-missing_questionnaires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean the listener data\n",
    "number_segments = nrow(speaker_segment_list)\n",
    "for (next_row in seq_along(1:number_segments)){\n",
    "    \n",
    "    # track how many complete listeners we have for each segment\n",
    "    complete_listeners = 0\n",
    "    \n",
    "    # isolate the data for this topic\n",
    "    next_segment = speaker_segment_list[next_row,]\n",
    "    next_topic = next_segment$topic\n",
    "    next_speaker = next_segment$speaker\n",
    "    speaker_side = as.character(next_segment$side)\n",
    "    next_topic_df = dplyr::filter(speaker_df,\n",
    "                                  speaker==next_speaker,\n",
    "                                  topic==next_topic,\n",
    "                                  side==speaker_side)\n",
    "    speaker_time = max(next_topic_df$time)\n",
    "    total_downsampled_samples = speaker_time*sampling_hz\n",
    "\n",
    "    # concatenate side and topic (if needed)\n",
    "    if (speaker_side=='only'){\n",
    "        topic_and_side = next_topic\n",
    "    } else {\n",
    "        topic_and_side = paste(next_topic, '-',\n",
    "                               speaker_side, sep='')\n",
    "    }\n",
    "    \n",
    "    # grab the files for a single topic\n",
    "    listener_gaze = list.files(listener_gaze_cleaned_path,\n",
    "                               recursive=FALSE, \n",
    "                               full.names=TRUE,\n",
    "                               pattern=as.character(topic_and_side))\n",
    "    \n",
    "    # cycle through the listeners\n",
    "    for (next_listener in listener_gaze){\n",
    "        \n",
    "        # only continue if we have their questionnaire data\n",
    "        listener_ID = strapply(next_listener, \"-([[:digit:]]*).csv\", c)[[1]]\n",
    "        listener_questionnaire_basename = paste0(listener_ID,\n",
    "                                                 '_questionnaire.csv')\n",
    "        listener_questionnaire_file = file.path(listener_responses_cleaned_path,\n",
    "                                                listener_questionnaire_basename)\n",
    "        if (file.exists(listener_questionnaire_file)){\n",
    "            \n",
    "            # find their gaze files\n",
    "            listener_gaze_path = list.files(listener_gaze_cleaned_path,\n",
    "                                           full.names=TRUE,\n",
    "                                           pattern=paste('gaze_trial-',topic_and_side,\n",
    "                                                         '.*',\n",
    "                                                         as.character(listener_ID),\n",
    "                                                         sep=''))\n",
    "            \n",
    "            # read in the file and downsample\n",
    "            listener_gaze_data = read.csv(listener_gaze_path[1],\n",
    "                                          stringsAsFactors=FALSE) %>%\n",
    "                mutate(time = round((Time - min(Time))/1000000,\n",
    "                                    sampling_round_time))\n",
    "            \n",
    "            # identify the participant's segment duration\n",
    "            listener_segment_time = max(listener_gaze_data$time)\n",
    "            total_listener_slices = listener_segment_time*sampling_hz\n",
    "            \n",
    "            # only move forward if we have opinion data on this topic\n",
    "            listener_questionnaire = read.table(listener_questionnaire_file,\n",
    "                                         sep=',', header=TRUE)\n",
    "            listener_opinions = dplyr::filter(listener_questionnaire, \n",
    "                                              Topic==as.character(topic_and_side))\n",
    "            \n",
    "            # identify user demographics and survey data\n",
    "            listener_gender = dplyr::filter(listener_questionnaire,\n",
    "                                             Source=='gender')$Answer\n",
    "            listener_natlang = set_concat_opinion(listener_questionnaire, \n",
    "                                                 'Answer',\n",
    "                                                 'Source',\n",
    "                                                 'nativelang')\n",
    "            listener_age = dplyr::filter(listener_questionnaire,\n",
    "                                             Source=='age')$Answer\n",
    "            listener_agree = set_opinion(listener_opinions, \n",
    "                                           'Answer',\n",
    "                                           'Source',\n",
    "                                           'agree')\n",
    "            \n",
    "            # only continue if listeners' trials were roughly the same \n",
    "            if (abs(speaker_time - listener_segment_time)<.2) {\n",
    "                if (nrow(listener_opinions)>0){\n",
    "\n",
    "                    # rename old variables\n",
    "                    setnames(listener_gaze_data,\n",
    "                             old=old_rename_columns, \n",
    "                             new=new_rename_columns)\n",
    "\n",
    "                    # clean up the data\n",
    "                    listener_gaze_data = listener_gaze_data %>% ungroup() %>%\n",
    "                    \n",
    "                        # add demographics\n",
    "                        select(one_of(listener_keep_columns)) %>%\n",
    "                        mutate(topic = next_topic) %>%\n",
    "                        mutate(listener = listener_ID) %>%\n",
    "                        mutate(side = speaker_side) %>%\n",
    "\n",
    "                        # keep fixations for real AOIs\n",
    "                        dplyr::filter(r_event=='Fixation') %>%\n",
    "                        dplyr::filter(r_aoi!='White Space' & r_aoi!='-') %>%\n",
    "\n",
    "                        # remove duplicate time slices\n",
    "                        group_by(time, listener) %>%\n",
    "                        slice(1) %>% ungroup()\n",
    "                    \n",
    "                    # if we still have data, process it\n",
    "                    if (nrow(listener_gaze_data)>0){\n",
    "                                                \n",
    "                        # pad the time variable\n",
    "                        listener_datatable = as.data.table(listener_gaze_data)\n",
    "                        setkey(listener_datatable, listener, time)\n",
    "                        time_slices = CJ(listener_ID,\n",
    "                                         seq(0,\n",
    "                                             listener_segment_time,\n",
    "                                             by=sampling_fraction)) %>%\n",
    "                            mutate(V2 = round(V2, sampling_round_time))\n",
    "\n",
    "                        # merge with gaze data\n",
    "                        listener_gaze_data = listener_gaze_data %>% ungroup() %>%\n",
    "\n",
    "                            # merge with time segments\n",
    "                            full_join(., time_slices,\n",
    "                                              by=c('listener'='V1',\n",
    "                                                   'time'='V2')) %>%\n",
    "                            arrange(time) %>%\n",
    "                        \n",
    "                            # add metadata\n",
    "                            mutate(topic = next_topic) %>%\n",
    "                            mutate(listener = listener_ID) %>%\n",
    "                            mutate(side = speaker_side) %>%\n",
    "\n",
    "                            # add survey responses\n",
    "                            mutate(gender = listener_gender) %>%\n",
    "                            mutate(age = listener_age) %>%\n",
    "                            mutate(native_lang = listener_natlang) %>%\n",
    "                            mutate(agree = listener_agree)\n",
    "                                \n",
    "                        # yell at us if we have more than one row per slice\n",
    "                        if (nrow(listener_gaze_data)!=(total_listener_slices+1)){\n",
    "                            print(paste('`', topic_and_side, '`: Listener ', listener_ID,\n",
    "                                        \" Error -- improper time slicing\", sep=\"\"))\n",
    "                        }\n",
    "\n",
    "                        # save cleaned dataframe to file\n",
    "                        outfile_basename = paste0('gaze_trial-',\n",
    "                                                  topic_and_side,'-',\n",
    "                                                  listener_ID,'.csv')\n",
    "                        outfile_name = file.path(listener_dataframe_path,\n",
    "                                                 outfile_basename)\n",
    "                        write.table(listener_gaze_data, \n",
    "                                    outfile_name,\n",
    "                                    append=FALSE, sep=',', row.names=FALSE)\n",
    "                        \n",
    "                        # get counts of all missing data\n",
    "                        next_missing_data = listener_gaze_data %>% ungroup() %>%\n",
    "                            select(one_of(listener_keep_columns)) %>%\n",
    "                            group_by(r_event) %>%\n",
    "                            summarize(counts = n())\n",
    "\n",
    "                        # update our complete listener count\n",
    "                        complete_listeners = complete_listeners + 1\n",
    "                        \n",
    "                    } else {\n",
    "                    \n",
    "                        # we only have blinking/missing event data\n",
    "                        print(paste('`', topic_and_side, '`: Listener ', listener_ID,\n",
    "                                    ' not added (entirely blink/missing event data)', \n",
    "                                    sep=\"\"))\n",
    "                        \n",
    "                        # update the proportion missing\n",
    "                        next_missing_data = data.frame(\n",
    "                                            counts = total_downsampled_samples,\n",
    "                                            r_event = 'all_missing')\n",
    "                    }\n",
    "                    \n",
    "                    # add all missing data for listener to same dataframe\n",
    "                    next_missing_data = next_missing_data %>% ungroup() %>%\n",
    "                            mutate(listener = listener_ID) %>%\n",
    "                            mutate(topic = next_topic) %>%\n",
    "                            mutate(side = speaker_side) %>%\n",
    "                            mutate(speaker_duration_samples = total_downsampled_samples) %>%\n",
    "                            mutate(proportion = counts/speaker_duration_samples)\n",
    "                    \n",
    "                    # append individual missing data stats to group dataframe\n",
    "                    missing_listener_data = rbind.data.frame(missing_listener_data,\n",
    "                                                             next_missing_data)\n",
    "                    \n",
    "                } else {\n",
    "                    # we don't have the data for the listener's opinion\n",
    "                    print(paste('`',topic_and_side, '`: Listener ', listener_ID,\n",
    "                                ' not added (opinion not found)', sep=\"\"))\n",
    "                    \n",
    "                    # store the dataframe\n",
    "                    next_missing_opinion = data.frame(listener = listener_ID,\n",
    "                                                      topic = next_topic,\n",
    "                                                      side = speaker_side)\n",
    "                    missing_opinion_listeners = rbind.data.frame(missing_opinion_listeners,\n",
    "                                                                 next_missing_opinion)\n",
    "                }\n",
    "            } else {\n",
    "                # the listener's time differed from the monologue\n",
    "                print(paste('`',topic_and_side,'`: Listener ', listener_ID,\n",
    "                            ' not added (too short/long trial; discrepancy = ',\n",
    "                            round(abs(speaker_time - max(listener_gaze_data$time)),\n",
    "                                 2), ' sec)', sep=\"\"))\n",
    "                \n",
    "                # store the dataframe\n",
    "                next_unbalanced_listener = data.frame(listener = listener_ID,\n",
    "                                                      topic = next_topic,\n",
    "                                                      side = speaker_side)\n",
    "                unbalanced_listeners = rbind.data.frame(unbalanced_listeners,\n",
    "                                                        next_unbalanced_listener)\n",
    "            }\n",
    "        }else{ \n",
    "            # we can't find the questionnaire\n",
    "            print(paste('`',topic_and_side,'`: Listener ', listener_ID,\n",
    "                        ' not added (questionnaire not found)',sep=\"\"))\n",
    "            \n",
    "            # store the dataframe\n",
    "            next_missing_questionnaire = data.frame(listener = listener_ID,\n",
    "                                                    topic = next_topic,\n",
    "                                                    side = speaker_side)\n",
    "            missing_questionnaire_listeners = rbind.data.frame(missing_questionnaire_listeners,\n",
    "                                                               next_missing_questionnaire)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # print update\n",
    "    print(paste(\"`\", topic_and_side, \"`: \", complete_listeners,\n",
    "                \" complete listeners added\", sep=\"\"))\n",
    "}\n",
    "\n",
    "# save dataframes to file if the tables have rows\n",
    "if (nrow(missing_listener_data)>0){\n",
    "    write.table(missing_listener_data,\n",
    "                missing_listener_file,\n",
    "                append=FALSE, sep=',' ,row.names=FALSE)\n",
    "}\n",
    "if (nrow(unbalanced_listeners)>0){\n",
    "    write.table(unbalanced_listeners,\n",
    "                unbalanced_listener_file,\n",
    "                append=FALSE, sep=',', row.names=FALSE)\n",
    "}\n",
    "if (nrow(missing_opinion_listeners)>0){\n",
    "    write.table(missing_opinion_listeners,\n",
    "                missing_opinion_file,\n",
    "                append=FALSE, sep=',', row.names=FALSE)\n",
    "}\n",
    "if (nrow(missing_questionnaire_listeners)>0){\n",
    "    write.table(missing_questionnaire_listeners,\n",
    "                missing_opinion_file,\n",
    "                append=FALSE, sep=',', row.names=FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unusable segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll figure out which segments have\n",
    "enough data to be considered for inclusion. Included\n",
    "segments must been missing no more than 30% of\n",
    "gaze samples for that segment and must have a\n",
    "record of their post-listening opinion for that segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create structure for unusable segments\n",
    "unusable_segment_df = data.frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify segments with missing opinions, unbalanced time, or missing questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (file.exists(missing_opinion_file)){\n",
    "\n",
    "    # load in the missing opinion table\n",
    "    missing_opinion = read.table(missing_opinion_file,\n",
    "                                 sep=',',\n",
    "                                 header=TRUE) %>%\n",
    "        mutate(reason = 'missing_opinion')\n",
    "    \n",
    "    # append to dataframe\n",
    "    unusable_segment_df = rbind.data.frame(unusable_segment_df,\n",
    "                                           missing_opinion)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (file.exists(missing_questionnaire_file)){\n",
    "\n",
    "    # load in the missing questionnaire table\n",
    "    missing_questionnaires = read.table(missing_questionnaire_file,\n",
    "                                        sep=',',\n",
    "                                        header=TRUE) %>%\n",
    "        mutate(reason = 'missing_questionnaire')\n",
    "    \n",
    "    # append to dataframe\n",
    "    unusable_segment_df = rbind.data.frame(unusable_segment_df,\n",
    "                                           missing_questionnaires)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (file.exists(unbalanced_listener_file)){\n",
    "\n",
    "    # load in the unbalanced segments table\n",
    "    unbalanced_listeners = read.table(unbalanced_listener_file,\n",
    "                                      sep=',',\n",
    "                                      header=TRUE) %>%\n",
    "        mutate(reason = 'unbalanced_listener')\n",
    "    \n",
    "    # append to dataframe\n",
    "    unusable_segment_df = rbind.data.frame(unusable_segment_df,\n",
    "                                           unbalanced_listeners)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify segments with more than 30% missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in the missing data table\n",
    "missing_data = read.table(missing_listener_file,\n",
    "                          sep=',',\n",
    "                          header=TRUE) %>%\n",
    "\n",
    "    # grab segments with more than 30% missing\n",
    "    dplyr::filter((is.na(r_event) & proportion>.3) |\n",
    "                  r_event=='all_missing') %>%\n",
    "\n",
    "    # remove unneeded columns and add reason\n",
    "    select(listener, topic, side) %>%\n",
    "    mutate(reason = 'excessive_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add to list\n",
    "unusable_segment_df = rbind.data.frame(unusable_segment_df,\n",
    "                                       missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unusable files from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# figure out which files we'll be removing\n",
    "unusable_segment_df = unusable_segment_df %>% ungroup() %>%\n",
    "\n",
    "    # identify base path and file name\n",
    "    mutate(segment_path = listener_dataframe_path) %>%\n",
    "    mutate(segment_name = ifelse(side=='only',\n",
    "                                 paste0('gaze_trial-',\n",
    "                                        topic,\n",
    "                                        '-',\n",
    "                                        listener,\n",
    "                                        '.csv'),\n",
    "                                 paste0('gaze_trial-',\n",
    "                                        topic,\n",
    "                                        '-',\n",
    "                                        side,\n",
    "                                        '-',\n",
    "                                        listener,\n",
    "                                        '.csv')\n",
    "                                )) %>%\n",
    "\n",
    "    # construct full path\n",
    "    mutate(segment_file = file.path(segment_path,\n",
    "                                    segment_name)) %>%\n",
    "\n",
    "    # drop now-unneeded variables\n",
    "    select(-segment_path, -segment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that the segments actually existed before deleting\n",
    "delete_segments = unusable_segment_df$segment_file\n",
    "delete_segments = delete_segments[file.exists(delete_segments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove them (silently)\n",
    "file.remove(delete_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save list of unusable segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "write.table(unusable_segment_df,\n",
    "            file.path(processed_data_path,\n",
    "                      'listener-unusable_segments.csv'),\n",
    "            append=FALSE,sep=',',row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-recurrence quantification analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cross-recurrence between speakers and listeners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reads in the files created in the previous section and \n",
    "performs CRQA over each speaker-listener pair. The results are then \n",
    "saved to a dataframe for future use in statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if we don't have a CRQA results folder, make it\n",
    "crqa_results_path = file.path(processed_data_path,\n",
    "                             'crqa-data-results')\n",
    "if (!dir.exists(crqa_results_path)){\n",
    "    dir.create(crqa_results_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty data frame\n",
    "crqa_frame = data.frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify all the speaker files\n",
    "speaker_gaze_files = list.files(speaker_dataframe_path, \n",
    "                                recursive=FALSE,\n",
    "                                full.names=TRUE,\n",
    "                                pattern=\"\\\\d+.*\\\\.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform CRQA over every speaker-listener pair\n",
    "for (next_speaker_gaze in speaker_gaze_files){\n",
    "    \n",
    "    # read speaker file\n",
    "    speaker_data = read.csv(next_speaker_gaze,\n",
    "                      sep=',',\n",
    "                      stringsAsFactors=FALSE,\n",
    "                      header=TRUE,\n",
    "                      skipNul = TRUE)\n",
    "    \n",
    "    # grab topic and ID\n",
    "    speaker_ID = unique(speaker_data$speaker)\n",
    "    speaker_topic = unique(speaker_data$topic)\n",
    "    speaker_side = unique(speaker_data$side)\n",
    "    \n",
    "    # create a temporary data frame for this one topic\n",
    "    temp_crqa_frame = data.frame()\n",
    "\n",
    "    # keep only the variables we need for CRQA\n",
    "    speaker_data = speaker_data %>%\n",
    "        select(one_of(crqa_compare_columns)) %>%\n",
    "        dplyr::rename(speaker_aoi = r_aoi)\n",
    "    \n",
    "    # identify speaker's topic and side for file name\n",
    "    if (speaker_side == 'only') {\n",
    "        topic_and_side = speaker_topic        \n",
    "    } else {\n",
    "        topic_and_side = paste(speaker_topic,\n",
    "                               '-',\n",
    "                               speaker_side,\n",
    "                               sep='')\n",
    "    }\n",
    "    \n",
    "    # get the files for this topic and side\n",
    "    listener_gaze_files = list.files(listener_dataframe_path, \n",
    "                                     pattern = (topic_and_side), \n",
    "                                     full.names = TRUE)\n",
    "    \n",
    "    # cycle through target listener gaze files\n",
    "    for (next_listener in listener_gaze_files){\n",
    "\n",
    "        # load in the next listener's data\n",
    "        listener_data = read.csv(next_listener,\n",
    "                                 stringsAsFactors=FALSE, \n",
    "                                 sep=',', \n",
    "                                 header=TRUE, \n",
    "                                 skipNul = TRUE)\n",
    "        \n",
    "        # grab listener questionnaire info \n",
    "        listener_questionnaire = listener_data %>%\n",
    "            select(one_of(crqa_questionnaire_columns)) %>%\n",
    "            distinct()\n",
    "            \n",
    "        # keep only the listener bits we need\n",
    "        listener_data = listener_data %>%\n",
    "            select(one_of(crqa_compare_columns)) %>%\n",
    "            dplyr::rename(listener_aoi = r_aoi)\n",
    "        \n",
    "        # merge with speaker data and refactor missing\n",
    "        both_data = full_join(speaker_data,\n",
    "                              listener_data,\n",
    "                              by='time') %>%\n",
    "            mutate(listener_aoi = ifelse(is.na(listener_aoi),\n",
    "                                         98,\n",
    "                                         listener_aoi)) %>%\n",
    "            mutate(speaker_aoi = ifelse(is.na(speaker_aoi),\n",
    "                                        99,\n",
    "                                        speaker_aoi))\n",
    "        \n",
    "        # run categorical CRQA\n",
    "        gaze_drp = drpdfromts(both_data$speaker_aoi,\n",
    "                              both_data$listener_aoi,\n",
    "                              ws=win_size,\n",
    "                              datatype=\"categorical\")\n",
    "\n",
    "        # correct maxlag and maxrec if missing data\n",
    "        if (length(gaze_drp$maxlag)==0){\n",
    "            \n",
    "            # grab the maximum rec and lag that aren't missing\n",
    "            crqa_remaining = gaze_drp$profile[!is.na(gaze_drp$profile)]\n",
    "            gaze_drp$maxrec = max(crqa_remaining)\n",
    "            gaze_drp$maxlag = match(gaze_drp$maxrec,\n",
    "                                    gaze_drp$profile)\n",
    "        }\n",
    "\n",
    "        # save to dataframes\n",
    "        next_crqa = data.frame('speaker' = speaker_ID,\n",
    "                               'listener' = listener_questionnaire$listener,\n",
    "                               'topic' = speaker_topic,\n",
    "                               'side' = speaker_side,\n",
    "                               'topic_and_side' = topic_and_side,\n",
    "                               't' = -win_size:win_size, \n",
    "                               'r' = gaze_drp$profile,\n",
    "                               'maxrec' = gaze_drp$maxrec,\n",
    "                               'maxlag' = gaze_drp$maxlag[1], \n",
    "                               'agree' = listener_questionnaire$agree,\n",
    "                               'gender' = listener_questionnaire$gender,\n",
    "                               'native_lang' = listener_questionnaire$native_lang,\n",
    "                               'age' = listener_questionnaire$age,\n",
    "                               row.names=NULL)\n",
    "        temp_crqa_frame = rbind.data.frame(temp_crqa_frame,\n",
    "                                           next_crqa)\n",
    "    }\n",
    "    \n",
    "    # create filename for this topic's dataframe\n",
    "    temp_out_filename = paste0('crqa_data-',\n",
    "                               topic_and_side,'.csv')\n",
    "    \n",
    "    # save this topic's dataframe\n",
    "    write.table(temp_crqa_frame,\n",
    "                file.path(crqa_results_path,\n",
    "                          temp_out_filename),\n",
    "                append=FALSE,\n",
    "                sep=',',\n",
    "                row.names=FALSE)\n",
    "    \n",
    "    # append this topic's dataframe to master dataframe\n",
    "    crqa_frame = rbind.data.frame(crqa_frame,\n",
    "                                  temp_crqa_frame)\n",
    "    \n",
    "    # print update\n",
    "    print(paste(\"`\", topic_and_side, \"`: CRQA complete\", sep=\"\"))\n",
    "}\n",
    "\n",
    "# save master CRQA frame\n",
    "write.table(crqa_frame,\n",
    "            file.path(crqa_results_path,'crqa_data-all_topics.csv'),\n",
    "            append=FALSE, sep=',', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create baseline data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll create the surrogate baselines\n",
    "to establish the degree to which we would expect gaze\n",
    "coordination to occur by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if we don't have a baseline folder, make it\n",
    "new_baseline_dirname = paste0('crqa-baseline-',\n",
    "                               as.numeric(Sys.time()[1]))\n",
    "new_baseline_path = file.path(processed_data_path,\n",
    "                              new_baseline_dirname)\n",
    "if (!dir.exists(new_baseline_path)){\n",
    "    dir.create(new_baseline_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify all the speaker files\n",
    "speaker_gaze_files = list.files(speaker_dataframe_path, \n",
    "                                recursive=FALSE,\n",
    "                                full.names=TRUE,\n",
    "                                pattern=\"\\\\d+.*\\\\.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty data frame\n",
    "crqa_baseline = data.frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform CRQA over surrogate data\n",
    "for (next_speaker_gaze in speaker_gaze_files){\n",
    "    \n",
    "    # set seed for reproducibility\n",
    "    set.seed(111)\n",
    "    \n",
    "    # read speaker file\n",
    "    speaker_data = read.csv(next_speaker_gaze,\n",
    "                            sep=',',\n",
    "                            stringsAsFactors=FALSE,\n",
    "                            header=TRUE,\n",
    "                            skipNul = TRUE)\n",
    "\n",
    "    # grab topic and ID\n",
    "    speaker_ID = unique(speaker_data$speaker)\n",
    "    speaker_topic = unique(speaker_data$topic)\n",
    "    speaker_side = unique(speaker_data$side)\n",
    "    \n",
    "    # create a temporary data frame for this one topic\n",
    "    temp_crqa_baseline = data.frame()\n",
    "\n",
    "    # keep only the variables we need for CRQA\n",
    "    speaker_data = speaker_data %>%\n",
    "        select(one_of(crqa_compare_columns)) %>%\n",
    "        dplyr::rename(speaker_aoi = r_aoi)\n",
    "    \n",
    "    # identify speaker's topic and side for file name\n",
    "    if (speaker_side == 'only') {\n",
    "        topic_and_side = speaker_topic        \n",
    "    } else {\n",
    "        topic_and_side = paste(speaker_topic,\n",
    "                               '-',\n",
    "                               speaker_side,\n",
    "                               sep='')\n",
    "    }\n",
    "    \n",
    "    # get the files for this topic and side\n",
    "    listener_gaze_files = list.files(listener_dataframe_path, \n",
    "                                     pattern=(topic_and_side),\n",
    "                                     full.names=TRUE)\n",
    "    \n",
    "    # re-create surrogate dyad and run again\n",
    "    for (next_listener in listener_gaze_files){\n",
    "        \n",
    "        # load in the next listener's data\n",
    "        listener_data = read.csv(next_listener,\n",
    "                                 stringsAsFactors=FALSE, \n",
    "                                 sep=',', \n",
    "                                 header=TRUE, \n",
    "                                 skipNul = TRUE)\n",
    "\n",
    "        # grab listener questionnaire info \n",
    "        listener_questionnaire = listener_data %>%\n",
    "            select(one_of(crqa_questionnaire_columns)) %>%\n",
    "            distinct()\n",
    "\n",
    "        # keep only the listener bits we need\n",
    "        listener_data = listener_data %>%\n",
    "            select(one_of(crqa_compare_columns)) %>%\n",
    "            dplyr::rename(listener_aoi = r_aoi)\n",
    "        \n",
    "        # merge with speaker data and refactor missing\n",
    "        both_data = full_join(speaker_data,\n",
    "                              listener_data,\n",
    "                              by='time') %>%\n",
    "            mutate(listener_aoi = ifelse(is.na(listener_aoi),\n",
    "                                         98,\n",
    "                                         listener_aoi)) %>%\n",
    "            mutate(speaker_aoi = ifelse(is.na(speaker_aoi),\n",
    "                                        99,\n",
    "                                        speaker_aoi))\n",
    "\n",
    "        # shuffle the dyad multiple times\n",
    "        for (run in 1:10){\n",
    "            \n",
    "            # shuffle listener's data\n",
    "            both_data = both_data %>%\n",
    "                mutate(listener_aoi = sample(listener_aoi))\n",
    "\n",
    "            # run categorical CRQA\n",
    "            gaze_drp = drpdfromts(both_data$speaker_aoi,\n",
    "                                  both_data$listener_aoi,\n",
    "                                  ws=win_size,\n",
    "                                  datatype=\"categorical\")\n",
    "\n",
    "            # correct maxlag and maxrec if missing data\n",
    "            if (length(gaze_drp$maxlag)==0){\n",
    "\n",
    "                # grab the maximum rec and lag that aren't missing\n",
    "                crqa_remaining = gaze_drp$profile[!is.na(gaze_drp$profile)]\n",
    "                gaze_drp$maxrec = max(crqa_remaining)\n",
    "                gaze_drp$maxlag = match(gaze_drp$maxrec,\n",
    "                                        gaze_drp$profile)\n",
    "            }\n",
    "\n",
    "            # save to dataframes\n",
    "            next_baseline = data.frame('speaker' = speaker_ID,\n",
    "                                       'listener' = listener_questionnaire$listener,\n",
    "                                       'topic_and_side' = topic_and_side,\n",
    "                                       't' = -win_size:win_size, \n",
    "                                       'r' = gaze_drp$profile,\n",
    "                                       'maxrec' = gaze_drp$maxrec,\n",
    "                                       'maxlag' = gaze_drp$maxlag[1], \n",
    "                                       'agree' = listener_questionnaire$agree,\n",
    "                                       'gender' = listener_questionnaire$gender,\n",
    "                                       'native_lang' = listener_questionnaire$native_lang,\n",
    "                                       'age' = listener_questionnaire$age,\n",
    "                                       'data' = 'baseline',\n",
    "                                       'run' = run,\n",
    "                                       row.names=NULL)\n",
    "            temp_crqa_baseline = rbind.data.frame(temp_crqa_baseline,\n",
    "                                                  next_baseline)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # create filename for this topic's dataframe\n",
    "    temp_out_filename = paste0('crqa_baseline_data-',\n",
    "                               topic_and_side,'.csv')\n",
    "    \n",
    "    # save this topic's dataframe\n",
    "    write.table(temp_crqa_baseline,\n",
    "                file.path(new_baseline_path,\n",
    "                          temp_out_filename),\n",
    "                append=FALSE,\n",
    "                sep=',',\n",
    "                row.names=FALSE)\n",
    "    \n",
    "    # append this topic's dataframe to master dataframe\n",
    "    crqa_baseline = rbind.data.frame(crqa_baseline,\n",
    "                                     temp_crqa_baseline)\n",
    "    \n",
    "    # print update\n",
    "    print(paste(\"`\", topic_and_side, \"`: Surrogate complete\", sep=\"\"))\n",
    "}\n",
    "\n",
    "# save master baseline frame\n",
    "final_out_file = file.path(new_baseline_path,\n",
    "                           'crqa_baseline_data-all_topics.csv')\n",
    "write.table(crqa_baseline,\n",
    "            final_out_file,\n",
    "            append=FALSE, sep=',', row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create analysis and plotting dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll first create first- and second-order\n",
    "orthogonal variables for lag (`t`) and then create separate \n",
    "dataframes to analyze and plot the data. The *analysis \n",
    "dataframe* will include all main terms and manually generated \n",
    "interaction terms between the target terms, all of which will\n",
    "then be centered and standardized so that we can interpret model \n",
    "estimates as effect sizes. The *plotting dataframe* will \n",
    "contain only the main terms and will be neither centered\n",
    "nor standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bind real and baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify CRQA results path\n",
    "crqa_results_path = file.path(processed_data_path,\n",
    "                             'crqa-data-results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get most recent baseline directory\n",
    "processed_directories = list.dirs(processed_data_path)\n",
    "all_baseline_directories = processed_directories[\n",
    "        !is.na(str_match(processed_directories,'baseline'))]\n",
    "target_baseline_path = file.path(tail(all_baseline_directories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in real data\n",
    "real_crqa_frame = read.table(file.path(crqa_results_path,\n",
    "                                       'crqa_data-all_topics.csv'),\n",
    "                             sep=',',\n",
    "                             header=TRUE,\n",
    "                             skipNul=TRUE) %>%\n",
    "    mutate(data = 'real') %>%\n",
    "    select(-topic, -side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in baseline data\n",
    "baseline_crqa_frame = read.table(file.path(target_baseline_path,\n",
    "                                       'crqa_baseline_data-all_topics.csv'),\n",
    "                             sep=',',\n",
    "                             header=TRUE,\n",
    "                             skipNul=TRUE) %>%\n",
    "    select(-run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a joint dataframe\n",
    "crqa_frame = rbind.data.frame(real_crqa_frame,\n",
    "                              baseline_crqa_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create numerical versions of most text variables\n",
    "crqa_frame = crqa_frame %>% ungroup() %>%\n",
    "    \n",
    "    # convert \"agree\" to binary (1=agree, 0=disagree)\n",
    "    mutate(agree = as.numeric(agree)) %>%\n",
    "    mutate(agree = (agree > 2) * 1) %>%\n",
    "\n",
    "    # identify whether the topic is dominant- (0) or mixed-view (1)\n",
    "    mutate(viewtype = ifelse(topic_and_side %in% dominant_view_topics,\n",
    "                             0,\n",
    "                             1)) %>%\n",
    "    \n",
    "    # convert topic and side to number\n",
    "    mutate(topic_and_side = as.numeric(topic_and_side)) %>%\n",
    "    \n",
    "    # assign numbers to \"gender\" (alphabetical)\n",
    "    mutate(gender = ifelse(gender=='Female',\n",
    "                           0,\n",
    "                           ifelse(gender=='Male',\n",
    "                                  1,\n",
    "                                  2))) %>%\n",
    "\n",
    "    # assign number to \"native_lang\" (alphabetical)\n",
    "    mutate(native_lang = ifelse(native_lang=='English',\n",
    "                                0,\n",
    "                                ifelse(native_lang=='Spanish',\n",
    "                                       1,\n",
    "                                       2))) %>%\n",
    "\n",
    "    # set baseline to 0 and real data to 1\n",
    "    mutate(data = ifelse(data=='real',\n",
    "                         1,\n",
    "                         0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center dummy-coded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crqa_frame = crqa_frame %>% ungroup() %>%\n",
    "    \n",
    "    # contrast-code the binary variables\n",
    "    mutate(agree = agree - .5) %>% # agree=.5, disagree=-.5\n",
    "    mutate(viewtype = viewtype - .5) %>% # mixed=.5, dominant=-.5\n",
    "    mutate(data = data - .5) %>% # real=.5, baseline=-.5\n",
    "\n",
    "    # center gender and language\n",
    "    mutate(gender = as.numeric(scale(gender,\n",
    "                                     center=TRUE,\n",
    "                                     scale=FALSE))) %>%\n",
    "    mutate(native_lang = as.numeric(scale(native_lang,\n",
    "                                          center=TRUE,\n",
    "                                          scale=FALSE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crqa_frame = na.omit(crqa_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create polynomial time variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a polynomial term for time variable\n",
    "crqa_frame$t = crqa_frame$t + win_size + 1\n",
    "poly_t = poly((unique(crqa_frame$t)),\n",
    "              2)\n",
    "crqa_frame[, paste(\"ot\",1:2,sep = \"\")] = poly_t[crqa_frame$t,\n",
    "                                                1:2]\n",
    "crqa_frame$t = crqa_frame$t - win_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that we don't have any 0s in ot1 or ot2\n",
    "crqa_frame = crqa_frame %>% ungroup() %>%\n",
    "    mutate(ot1 = ot1 + min(ot1) + 1) %>%\n",
    "    mutate(ot2 = ot2 + min(ot2) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a folder for analysis-ready data, if it doesn't exist\n",
    "if (!dir.exists(analysis_data_path)){\n",
    "    dir.create(analysis_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write.table(crqa_frame,\n",
    "            file.path(analysis_data_path,\n",
    "                      'oag-plotting_df.csv'),\n",
    "            append=FALSE,\n",
    "            sep=',',\n",
    "            row.names=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
