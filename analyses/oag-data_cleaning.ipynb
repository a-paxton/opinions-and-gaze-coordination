{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinions and Gaze: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the gaze and questionnaire \n",
    "data cleaning for \"Seeing the other side: Conflict and \n",
    "controversy increase gaze coordination\" (Paxton, Dale, \n",
    "& Richardson, *in preparation*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this file from scratch, you will need:\n",
    "* `data/01-input/`: Directory of experiment data, available for download from the project's ICPSR directory (link here). Due to data sensitivity (per the Institutional Review Board of the University of California, Merced), only researchers from ICPSR member institutions may access the data.\n",
    "    * `listener-gaze-raw/*.txt`: Listeners' raw gaze data\n",
    "    * `listener-responses-raw/`:\n",
    "        * `*.xml`: Listeners' trial order during experiment\n",
    "        * `*.tsv`: Listeners' post-experiment questionnaire data\n",
    "    * `speaker-gaze-raw/*.txt`: Speakers' raw gaze data\n",
    "    * `speaker-audio_outputs/*.csv`: Speaker's audio segment timing\n",
    "    * `speaker-segment_key.csv`: List of which speakers provided which stimulus segments\n",
    "* `supplementary-code/`: Directory of additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preliminaries](#Preliminaries)\n",
    "* [Listener gaze data](#Listener-gaze-data)\n",
    "    * [Convert listeners' raw SMI files](#Convert-listeners'-raw-SMI-files)\n",
    "    * [Concatenate multifile data from single listeners](#Concatenate-multifile-data-from-single-listeners)\n",
    "    * [Segment listeners' files by audio clip](#Segment-listeners'-files-by-audio-clip)\n",
    "* [Listener survey response data](#Listener-survey-response-data)\n",
    "* [Speaker gaze data](#Speaker-gaze-data)\n",
    "    * [Convert speakers' raw SMI files](#Convert-speakers'-raw-SMI-files)\n",
    "    * [Filter speaker gaze data to relevant topic](#Filter-speaker-gaze-data-to-relevant-topic)\n",
    "* [Clean up interim directories](#Clean-up-interim-directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written by**: A. Paxton (University of California, Berkeley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date last modified**: 28 March 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and set global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in bespoke functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-clean_responses.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-stimulus_order.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../supplementary-code/func-clean_gaze_data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or specify required paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input data file path\n",
    "input_data_path = os.path.join('../data/01-input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "cleaned_data_path = os.path.join('../data/02-data_cleaning')\n",
    "if not os.path.exists(cleaned_data_path):\n",
    "    os.makedirs(cleaned_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listener gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert listeners' raw SMI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all of our raw gaze data\n",
    "raw_listener_gaze_files = os.path.join(input_data_path,\n",
    "                                       'listener-gaze-raw/*.txt')\n",
    "gazeData = glob.glob(raw_listener_gaze_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "prepped_listener_path = os.path.join(cleaned_data_path,\n",
    "                                     'listener-gaze-prepped')\n",
    "if not os.path.exists(prepped_listener_path):\n",
    "    os.makedirs(prepped_listener_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandra/anaconda3/envs/py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/53362-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56653-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/51196-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58300-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/46060-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57295-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/44554-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/47734-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57040-redu_026_Trial084 Samples.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/52291-continued-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/53833-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57871 -redo-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58126-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/55900-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58396-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57856-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57061-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/46102-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57595-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/53437-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57160-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/55300-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56596-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58177-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56824-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57697-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/52351-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56866-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56692-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/52657-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56884-retry-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/54112-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58049-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58048-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/51190-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57928-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56644-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57586-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58435-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/53446-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/51076-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58303-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/55891-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/52291-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57082-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57040-redu-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57142-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58270-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57943-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58186-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57145-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/54190-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58132-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57340-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/43423-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58012-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56575-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/47794-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56827-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/49072-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56878-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56932-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/55168-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57982-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57541-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/58156-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57400-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57505-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/56959-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/listener-gaze-prepped/57538-smi-data.csv\n"
     ]
    }
   ],
   "source": [
    "# process each file\n",
    "for gazeFile in gazeData: clean_gaze_data(gazeFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate multifile data from single listeners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all our processed files' names\n",
    "processed_listener_files = os.path.join(prepped_listener_path,\n",
    "                                       '*.csv')\n",
    "processed_files = glob.glob(processed_listener_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify files with longer names than expected\n",
    "possible_multipart_files = [re.findall('\\d{5}', ID)[0] for ID \n",
    "                            in processed_files if len(ID)>48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify possible multipart files\n",
    "multipart_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in possible_multipart_files]\n",
    "multipart_ids = [re.findall('\\d{5}', mpf)[0] for mpf in multipart_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many times these odd IDs appear in our files\n",
    "from collections import Counter\n",
    "single_ids = [ID for ID in Counter(multipart_ids) if \n",
    "              Counter(multipart_ids)[ID]==1]\n",
    "multi_ids = [ID for ID in Counter(multipart_ids) if \n",
    "              Counter(multipart_ids)[ID]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the IDs that only occur once\n",
    "single_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in single_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the oddly named files with unique IDs\n",
    "for single_file in single_files:\n",
    "    participant_id = re.findall('\\d{5}', single_file)[0]\n",
    "    new_file_name = os.path.join(prepped_listener_path,\n",
    "                                 participant_id+'-smi-data.csv')\n",
    "    os.rename(single_file,new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the multi-part participant files\n",
    "multi_files = [gaze_id for gaze_id in processed_files if\n",
    "                  re.findall('\\d{5}', gaze_id)[0] in multi_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through all possibly duplicated participants\n",
    "for duplicate_id in multi_ids:\n",
    "    \n",
    "    # identify which files belong to this person\n",
    "    target_files = [next_file for next_file in processed_files if\n",
    "                      re.findall('\\d{5}', next_file)[0] == duplicate_id]\n",
    "    \n",
    "    # concatenate the files\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    for next_file in target_files:\n",
    "        concatenated_df = concatenated_df.append(\n",
    "                                            pd.read_csv(next_file)\n",
    "                                        ).reset_index(drop=True)\n",
    "        \n",
    "    # delete the old files\n",
    "    for next_file in target_files:\n",
    "        os.remove(next_file)\n",
    "        \n",
    "    # if there are any duplicated rows, just keep the first\n",
    "    concatenated_df = concatenated_df.drop_duplicates()\n",
    "    \n",
    "    # save the new file\n",
    "    new_concat_name = os.path.join(prepped_listener_path,\n",
    "                                   duplicate_id+'-smi-data.csv')\n",
    "    concatenated_df.to_csv(new_concat_name, sep=',', \n",
    "                           header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment listeners' files by audio clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "cleaned_listener_gaze_path = os.path.join(cleaned_data_path,\n",
    "                                          'listener-gaze-cleaned')\n",
    "if not os.path.exists(cleaned_listener_gaze_path):\n",
    "    os.makedirs(cleaned_listener_gaze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the prepped gaze files\n",
    "processed_listener_files = os.path.join(prepped_listener_path,\n",
    "                                       '*.csv')\n",
    "gazeCleaned = glob.glob(processed_listener_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which stimuli only had 1 audio clip\n",
    "single_stimuli = ['abortion',\n",
    "                  'gay-marriage',\n",
    "                  'legal-marijuana',\n",
    "                  'tax-rich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 58048 Exported.\n",
      "ERROR for ID 56644: Gaze Data for `death-penalty` Not Found.\n",
      "Participant 56644 Exported.\n",
      "Participant 56932 Exported.\n",
      "Participant 55300 Exported.\n",
      "Participant 57505 Exported.\n",
      "Participant 54112 Exported.\n",
      "Participant 58303 Exported.\n",
      "Participant 56866 Exported.\n",
      "Participant 58435 Exported.\n",
      "Participant 43423 Exported.\n",
      "Participant 44554 Exported.\n",
      "Participant 58186 Exported.\n",
      "Participant 55891 Exported.\n",
      "Participant 58049 Exported.\n",
      "Participant 47734 Exported.\n",
      "Participant 55168 Exported.\n",
      "Participant 56827 Exported.\n",
      "Participant 56878 Exported.\n",
      "Participant 57595 Exported.\n",
      "Participant 57538 Exported.\n",
      "Participant 57856 Exported.\n",
      "Participant 56575 Exported.\n",
      "Participant 54190 Exported.\n",
      "Participant 57295 Exported.\n",
      "Participant 47794 Exported.\n",
      "Participant 58126 Exported.\n",
      "Participant 57145 Exported.\n",
      "Participant 57142 Exported.\n",
      "Participant 58156 Exported.\n",
      "Participant 57160 Exported.\n",
      "Participant 49072 Exported.\n",
      "Participant 53362 Exported.\n",
      "Participant 53446 Exported.\n",
      "Participant 57586 Exported.\n",
      "ERROR for ID 52291: Gaze Data for `drinking-age` Not Found.\n",
      "Participant 52291 Exported.\n",
      "Participant 56692 Exported.\n",
      "Participant 46060 Exported.\n",
      "Participant 53437 Exported.\n",
      "Participant 55900 Exported.\n",
      "Participant 57061 Exported.\n",
      "Participant 58132 Exported.\n",
      "Participant 46102 Exported.\n",
      "Participant 57943 Exported.\n",
      "ERROR for ID 57040: Gaze Data for `death-penalty` Not Found.\n",
      "Participant 57040 Exported.\n",
      "Participant 51196 Exported.\n",
      "Participant 56959 Exported.\n",
      "Participant 56884 Exported.\n",
      "Participant 57871 Exported.\n",
      "Participant 58177 Exported.\n",
      "Participant 58012 Exported.\n",
      "Participant 51190 Exported.\n",
      "Participant 52657 Exported.\n",
      "Participant 58396 Exported.\n",
      "ERROR for ID 51076: Gaze Data for `drinking-age` Not Found.\n",
      "Participant 51076 Exported.\n",
      "Participant 57541 Exported.\n",
      "Participant 57340 Exported.\n",
      "Participant 57082 Exported.\n",
      "Participant 57400 Exported.\n",
      "Participant 58270 Exported.\n",
      "ERROR for ID 58300: Gaze Data for `death-penalty` Not Found.\n",
      "ERROR for ID 58300: Gaze Data for `junk-food-tax` Not Found.\n",
      "Participant 58300 Exported.\n",
      "Participant 56653 Exported.\n",
      "Participant 56596 Exported.\n",
      "Participant 53833 Exported.\n",
      "Participant 57697 Exported.\n",
      "ERROR for ID 56824: Gaze Data for `drinking-age` Not Found.\n",
      "ERROR for ID 56824: Gaze Data for `junk-food-tax` Not Found.\n",
      "ERROR for ID 56824: Gaze Data for `death-penalty` Not Found.\n",
      "Participant 56824 Exported.\n",
      "Participant 52351 Exported.\n",
      "Participant 57982 Exported.\n",
      "ERROR for ID 57928: Gaze Data for `death-penalty` Not Found.\n",
      "Participant 57928 Exported.\n"
     ]
    }
   ],
   "source": [
    "# cycle through the participants' data\n",
    "for nextGaze in gazeCleaned:\n",
    "    \n",
    "    # grab the next set of gaze data\n",
    "    gaze_df = pd.read_csv(nextGaze,sep=',')\n",
    "    particID = re.findall('\\d{5}',nextGaze)[0]\n",
    "    \n",
    "    # identify which stimuli are present in this participant's data\n",
    "    available_stimuli = list(set(gaze_df['Stimulus']))\n",
    "    pic_stimuli = [stim for stim in available_stimuli \n",
    "                       if len(re.findall(' Page',str(stim)))>0]\n",
    "    \n",
    "    # cycle through the available picture data\n",
    "    for next_pic in pic_stimuli:\n",
    "        \n",
    "        # grab the next picture data and the correct name for it\n",
    "        base_stim_name = next_pic.split(' ')[0]\n",
    "        instr_and_gaze_subset = gaze_df[gaze_df['Stimulus'].\\\n",
    "                                            str.contains(base_stim_name)\\\n",
    "                                       ].reset_index(drop=True)\n",
    "        gaze_subset = gaze_df[gaze_df['Stimulus'].\\\n",
    "                                  str.contains(next_pic)\\\n",
    "                             ].reset_index(drop=True)\n",
    "        \n",
    "        # if it's a unimodal issue, we don't have to worry about finding the second listening section\n",
    "        if base_stim_name in single_stimuli:\n",
    "            gaze_save_name = os.path.join(cleaned_listener_gaze_path,\n",
    "                                          'gaze_trial-'+base_stim_name+\n",
    "                                          '-'+particID+'.csv')\n",
    "            gaze_subset.to_csv(gaze_save_name, index=False)\n",
    "        \n",
    "        # if it is part of a two-opinion topic, make sure we have both topics, and then slice them\n",
    "        elif len(instr_and_gaze_subset['Stimulus'].unique())>2:\n",
    "            \n",
    "            # get first appearance of each stimulus\n",
    "            first_appearance = instr_and_gaze_subset.\\\n",
    "                                        groupby('Stimulus').\\\n",
    "                                        first().\\\n",
    "                                        sort_values(by='Time',ascending=True).\\\n",
    "                                        reset_index()\n",
    "            \n",
    "            # index the last time the participant saw the \"for\" and \"against\" instructions\n",
    "            against_instr = first_appearance['Time']\\\n",
    "                                [first_appearance['Stimulus'].\\\n",
    "                                         str.contains('-against.rtf')\\\n",
    "                                ].item()\n",
    "            for_instr = first_appearance['Time']\\\n",
    "                                [first_appearance['Stimulus'].\\\n",
    "                                         str.contains('-for.rtf')\\\n",
    "                                ].item()\n",
    "\n",
    "            # if the \"against\" instruction came first\n",
    "            if against_instr < for_instr:\n",
    "                against_listening = gaze_subset[gaze_subset['Time'] < for_instr]\n",
    "                for_listening = gaze_subset[gaze_subset['Time'] > for_instr]\n",
    "            \n",
    "            # otherwise, the \"for\" instruction came first\n",
    "            else:\n",
    "                for_listening = gaze_subset.loc[gaze_subset['Time'] < against_instr]\n",
    "                against_listening = gaze_subset.loc[gaze_subset['Time'] > against_instr]\n",
    "                \n",
    "            # create the file names\n",
    "            against_file = os.path.join(cleaned_listener_gaze_path,'gaze_trial-'\n",
    "                                        +base_stim_name+'-against-'+particID+'.csv')\n",
    "            for_file = os.path.join(cleaned_listener_gaze_path,'gaze_trial-'\n",
    "                                    +base_stim_name+'-for-'+particID+'.csv')\n",
    "            \n",
    "            # and then print it\n",
    "            against_listening.to_csv(against_file,index=False)\n",
    "            for_listening.to_csv(for_file,index=False)\n",
    "        \n",
    "        # if we don't have them both, let us know\n",
    "        else:\n",
    "            print \"ERROR for ID \"+particID+\": Gaze Data for `\"+base_stim_name+\"` Not Found.\"\n",
    "    \n",
    "    # print update\n",
    "    print \"Participant \"+particID+\" Exported.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listener survey response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through listener raw data\n",
    "raw_listener_response_path = os.path.join(input_data_path,\n",
    "                                          'listener-responses-raw/') \n",
    "listener_folders = glob.glob(os.path.join(raw_listener_response_path,\n",
    "                                          '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "cleaned_listener_response_path = os.path.join(cleaned_data_path,\n",
    "                                              'listener-responses-cleaned')\n",
    "if not os.path.exists(cleaned_listener_response_path):\n",
    "    os.makedirs(cleaned_listener_response_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab listener IDs associated with our gaze files\n",
    "unique_listeners = glob.glob(os.path.join(cleaned_listener_response_path,\n",
    "                                          '*.csv'))\n",
    "unique_listeners = list(set([re.findall('\\d{5}',f_name)[0] \n",
    "                             for f_name in unique_listeners]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify our rating categories\n",
    "l_ratings = ['rat-emot','passionate','know','convince','agree','common']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cycle through the listeners to grab their questionnaire data\n",
    "missing_files = pd.DataFrame()\n",
    "for listener in unique_listeners:\n",
    "\n",
    "    # identify the questionnaire and XML paths\n",
    "    q_tsv_data_file = glob.glob(os.path.join(raw_listener_response_path,\n",
    "                                             listener+'*.tsv'))\n",
    "    xml_data_file = glob.glob(os.path.join(raw_listener_response_path,\n",
    "                                           listener+'*.xml'))\n",
    "    \n",
    "    # if we've got a questionnaire, process it\n",
    "    if len(q_tsv_data_file)>0 and len(xml_data_file)>0:\n",
    "        \n",
    "        # if we only have 1 per participant...\n",
    "        if len(q_tsv_data_file)==1:\n",
    "            \n",
    "            # export TSV to a new file\n",
    "            q_csv_data_file = os.path.join(cleaned_listener_response_path,\n",
    "                                           listener+'_questionnaire.csv')\n",
    "            clean_q_df = clean_responses(listener,\n",
    "                                         q_tsv_data_file[0],\n",
    "                                         q_csv_data_file)\n",
    "            \n",
    "            # grab their XML data           \n",
    "            task_xml_data_file = open(xml_data_file[0], 'r').read()\n",
    "            l_stimulus_order = stimulus_order(task_xml_data_file)\n",
    "            l_stimulus_order = [re.sub('\\-(for|against)','',stimulus) \n",
    "                                if re.sub('\\-(for|against)','',stimulus) in single_stimuli \n",
    "                                else stimulus \n",
    "                                for stimulus in l_stimulus_order]\n",
    "\n",
    "            # add the task order data to the dataframe\n",
    "            clean_q_df['Topic'] = 'none'\n",
    "            for rating in l_ratings:\n",
    "                clean_q_df['Topic'].\\\n",
    "                        loc[clean_q_df['Source']==rating] \\\n",
    "                                = l_stimulus_order[0:len(\\\n",
    "                                                     clean_q_df['Topic'].\\\n",
    "                                                         loc[clean_q_df['Source']==rating])]\n",
    "\n",
    "            # then save it and report back\n",
    "            clean_q_df.to_csv(q_csv_data_file,index=False)\n",
    "            print('Listener ID '+str(listener)+' Questionnaire Data Exported.')\n",
    "            \n",
    "        # if we've got more than 1 file per participant...\n",
    "        else:\n",
    "            \n",
    "            # create an overarching dataframe and filename for the participant\n",
    "            combined_clean_q_df = pd.DataFrame()\n",
    "            q_csv_data_file = os.path.join(cleaned_listener_response_path,\n",
    "                                           listener+'_questionnaire.csv')\n",
    "            \n",
    "            # preserve their file names when first processing\n",
    "            for q_file in q_tsv_data_file:\n",
    "\n",
    "                # get the requisite files\n",
    "                next_ID = re.sub('_questionnaire.tsv', '', \n",
    "                                         os.path.basename(q_file))\n",
    "                xml_data_file = os.path.join(raw_listener_response_path,\n",
    "                                             next_ID+'-stimulus-log.xml')\n",
    "                \n",
    "                # clean up the TSV but DO NOT save\n",
    "                clean_q_df = clean_responses(next_ID, q_file, None)\n",
    "                \n",
    "                # process the XML data\n",
    "                task_xml_data_file = open(xml_data_file, 'r').read()\n",
    "                l_stimulus_order = stimulus_order(task_xml_data_file)\n",
    "                l_stimulus_order = [re.sub('\\-(for|against)','',stimulus) \n",
    "                                    if re.sub('\\-(for|against)','',stimulus) in single_stimuli \n",
    "                                    else stimulus \n",
    "                                    for stimulus in l_stimulus_order]\n",
    "\n",
    "                # add the task order data to the dataframe\n",
    "                clean_q_df['Topic'] = 'none'\n",
    "                for rating in l_ratings:\n",
    "                    clean_q_df['Topic'].\\\n",
    "                            loc[clean_q_df['Source']==rating] \\\n",
    "                                    = l_stimulus_order[0:len(\\\n",
    "                                                         clean_q_df['Topic'].\\\n",
    "                                                             loc[clean_q_df['Source']==rating])]\n",
    "                \n",
    "                # if this isn't the first dataset, remove the demographic survey data\n",
    "                if combined_clean_q_df.size!=0:\n",
    "                    clean_q_df = clean_q_df[clean_q_df['Topic']!='none']\n",
    "\n",
    "                # add to master dataframe    \n",
    "                combined_clean_q_df = combined_clean_q_df.append(clean_q_df)\n",
    "\n",
    "            # save it and report back\n",
    "            combined_clean_q_df.to_csv(q_csv_data_file,index=False)\n",
    "            print 'Listener ID '+str(listener)+ ' Questionnaire Data Exported.'\n",
    "\n",
    "    # if necessary files don't exist, let us know\n",
    "    else:\n",
    "        \n",
    "        # if we don't have the questionnaire:\n",
    "        if len(q_tsv_data_file)==0:\n",
    "            missing_files = missing_files.append({'listener': listener, \n",
    "                                  'missing_file': 'questionnaire_tsv'}, \n",
    "                                 ignore_index=True)\n",
    "            print 'ERROR: Listener ID '+str(listener)+' Questionnaire Data Not Found.'\n",
    "\n",
    "        # if we don't have the order data\n",
    "        if len(xml_data_file)==0:\n",
    "            missing_files = missing_files.append({'listener': listener, \n",
    "                                  'missing_file': 'stimulus_order_data'}, \n",
    "                                 ignore_index=True)\n",
    "            print 'ERROR: Listener ID '+str(listener)+' XML Data Not Found.'\n",
    "\n",
    "# save missing data to file\n",
    "missing_files.to_csv(os.path.join(cleaned_data_path,\n",
    "                                  'listeners-unadded-missing_files.csv'),\n",
    "                     index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker gaze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare speaker gaze data for only each target trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert speakers' raw SMI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab speakers' raw gaze data\n",
    "raw_speaker_gaze_path = os.path.join(input_data_path,\n",
    "                                     'speaker-gaze-raw')\n",
    "gazeData = glob.glob(os.path.join(raw_speaker_gaze_path,\n",
    "                                  '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new clean directory if it doesn't yet exist\n",
    "prepped_speaker_path = os.path.join(cleaned_data_path,\n",
    "                                    'speaker-gaze-prepped')\n",
    "if not os.path.exists(prepped_speaker_path):\n",
    "    os.makedirs(prepped_speaker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed SMI Data File: ../data/02-data_cleaning/speaker-gaze-prepped/44881-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/speaker-gaze-prepped/51916-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/speaker-gaze-prepped/53596-smi-data.csv\n",
      "Processed SMI Data File: ../data/02-data_cleaning/speaker-gaze-prepped/56386-smi-data.csv\n"
     ]
    }
   ],
   "source": [
    "# process each file\n",
    "for gazeFile in gazeData: clean_gaze_data(gazeFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter speaker gaze data to relevant topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new target directory if it doesn't yet exist\n",
    "cleaned_speaker_path = os.path.join(cleaned_data_path,\n",
    "                                    'speaker-gaze-cleaned')\n",
    "if not os.path.exists(cleaned_speaker_path):\n",
    "    os.makedirs(cleaned_speaker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data for speaker audio clips\n",
    "segment_key = pd.read_table(os.path.join(input_data_path,\n",
    "                                        'speaker-segment_key.csv'),\n",
    "                           sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to speakers' audio data\n",
    "speaker_audio_path = os.path.join(input_data_path,\n",
    "                                  'speaker-audio_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique speakers\n",
    "speaker_list = segment_key['speaker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker Data Exported: death-penalty (against)\n",
      "Speaker Data Exported: junk-food-tax (for)\n",
      "Speaker Data Exported: tax-rich (for)\n",
      "Speaker Data Exported: drinking-age (against)\n",
      "Speaker Data Exported: gay-marriage (for)\n",
      "Speaker Data Exported: junk-food-tax (against)\n",
      "Speaker Data Exported: legal-marijuana (for)\n",
      "Speaker Data Exported: abortion (for)\n",
      "Speaker Data Exported: death-penalty (for)\n",
      "Speaker Data Exported: drinking-age (for)\n"
     ]
    }
   ],
   "source": [
    "# cycle through the speakers\n",
    "for speaker in speaker_list:\n",
    "    \n",
    "    # figure out how many segments the speaker did\n",
    "    speaker_segments = segment_key[segment_key['speaker']==speaker].reset_index()\n",
    "    \n",
    "    # read in the speaker's data\n",
    "    prepped_gaze = pd.read_csv(os.path.join(prepped_speaker_path,\n",
    "                                            str(speaker)+\n",
    "                                            '-smi-data.csv'))\n",
    "    all_audio = pd.read_csv(os.path.join(speaker_audio_path,\n",
    "                                         str(speaker)+'-winnowed_samples.csv'))\n",
    "    all_audio['stim'] = all_audio['Stimulus'].replace(' Page 1','',regex=True)\n",
    "        \n",
    "    # cycle through the segments\n",
    "    for segment_number in range(0,speaker_segments.shape[0]):\n",
    "    \n",
    "        # grab the next row\n",
    "        next_segment = speaker_segments.iloc[segment_number]\n",
    "        topic = next_segment['topic']\n",
    "        side = next_segment['side']\n",
    "        speaker = str(speaker)\n",
    "\n",
    "        # grab only the times that correspond to our target trial\n",
    "        audio_times = all_audio[all_audio['stim']==topic]\n",
    "        start_time = min(audio_times['Time'])\n",
    "        end_time = max(audio_times['Time'])\n",
    "\n",
    "        # carve out the data between start_time and end_time and save only the columns we need\n",
    "        target_gaze = prepped_gaze.loc[prepped_gaze['Time']>=start_time]\n",
    "        target_gaze = target_gaze.loc[target_gaze['Time']<=end_time]\n",
    "        if topic in single_stimuli:\n",
    "            outname = os.path.join(cleaned_speaker_path,\n",
    "                                   'gaze_speaker-'+topic+'-'+speaker+'.csv')\n",
    "        else:\n",
    "            outname = os.path.join(cleaned_speaker_path,\n",
    "                                   'gaze_speaker-'+topic+'-'+side+'-'+speaker+'.csv')\n",
    "        # save the data\n",
    "        target_gaze.to_csv(outname,index=False)\n",
    "\n",
    "        # print out an update\n",
    "        print \"Speaker Data Exported: \"+topic+\" (\"+side+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up interim directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done, we can delete the interim files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(prepped_speaker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(prepped_listener_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
